{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec87b520-270a-46f8-9f1c-ce25508854ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4821331748397817#setting/sparkui/0212-035924-se9qcz78/driver-3821346366727538720\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3ef2fe6740>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark=SparkSession.builder.appName('PySpark Coding challenge').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1faf14ba-4416-4b1c-8cbe-40fabcbebb2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+------+---+-----+\n|  Name|Department|State|Salary|Age|Bonus|\n+------+----------+-----+------+---+-----+\n|  Sona|     Sales|   NY| 90000| 34|10000|\n|  Mona|     Sales|   NY| 86000| 56|20000|\n| Bannu|     Sales|   CA| 81000| 30|23000|\n|Chinnu|   Finance|   CA| 90000| 24|23000|\n| Sunny|   Finance|   CA| 99000| 40|24000|\n|  Raju|   Finance|   NY| 83000| 36|19000|\n|  ravi|   Finance|   NY| 79000| 53|15000|\n| Dhana| Marketing|   CA| 80000| 25|18000|\n|   Ram| Marketing|   NY| 91000| 50|21000|\n+------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe\n",
    "\n",
    "data=[(\"Sona\",\"Sales\",\"NY\",90000,34,10000),\n",
    "      (\"Mona\",\"Sales\",\"NY\",86000,56,20000),\n",
    "      (\"Bannu\",\"Sales\",\"CA\",81000,30,23000),\n",
    "      (\"Chinnu\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Sunny\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Raju\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"ravi\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Dhana\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Ram\",\"Marketing\",\"NY\",91000,50,21000)]\n",
    "\n",
    "columns=[\"Name\",\"Department\",\"State\",\"Salary\",\"Age\",\"Bonus\"]\n",
    "\n",
    "df1=spark.createDataFrame(data=data,schema=columns)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875bcd6a-9686-445e-97c7-667812fcdab0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- Age: long (nullable = true)\n |-- Bonus: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "############ Manipulating\n",
    "\n",
    "# 1) schema \n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179b17f3-507a-417b-9c11-7407b8dbc2fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+------+---+-----+\n|Name|Department|State|Salary|Age|Bonus|\n+----+----------+-----+------+---+-----+\n|Sona|     Sales|   NY| 90000| 34|10000|\n|Mona|     Sales|   NY| 86000| 56|20000|\n+----+----------+-----+------+---+-----+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 2) show only 2 top records\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd99e65-c87d-4793-b631-82a49ea5c818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n6\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Name', 'Department', 'State', 'Salary', 'Age', 'Bonus']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) count number of rows in Df\n",
    "print(df1.count())\n",
    "\n",
    "# 4) count of number of columns######## Dropping\n",
    "#1) Drop single Column From DataFrame\n",
    "\n",
    "# i)\n",
    "df1.drop(\"Age\").printSchema()\n",
    "df1.drop(\"Age\").show() \n",
    "print(len(df1.columns))\n",
    "\n",
    "# 5) names of columns\n",
    "df1.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07634a4-c868-4615-9034-f0d7f4f41276",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+------+---+-----+-----+\n|  Name|Department|State|Salary|Age|Bonus|Marks|\n+------+----------+-----+------+---+-----+-----+\n|  Sona|     Sales|   NY| 90000| 34|10000|   90|\n|  Mona|     Sales|   NY| 86000| 56|20000|   90|\n| Bannu|     Sales|   CA| 81000| 30|23000|   90|\n|Chinnu|   Finance|   CA| 90000| 24|23000|   90|\n| Sunny|   Finance|   CA| 99000| 40|24000|   90|\n|  Raju|   Finance|   NY| 83000| 36|19000|   90|\n|  ravi|   Finance|   NY| 79000| 53|15000|   90|\n| Dhana| Marketing|   CA| 80000| 25|18000|   90|\n|   Ram| Marketing|   NY| 91000| 50|21000|   90|\n+------+----------+-----+------+---+-----+-----+\n\n+----------+-----+\n|Department|State|\n+----------+-----+\n|     Sales|   NY|\n|     Sales|   NY|\n|     Sales|   CA|\n|   Finance|   CA|\n|   Finance|   CA|\n|   Finance|   NY|\n|   Finance|   NY|\n| Marketing|   CA|\n| Marketing|   NY|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 6) Adding new columns\n",
    "from pyspark.sql.functions import lit\n",
    "df1.withColumn(\"Marks\",lit(90)).show()\n",
    "\n",
    "# 7) select() operations\n",
    "df1.select(df1.Department,df1.State).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d12541c-5129-4a1a-b4ca-d971bb31fbe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- Bonus: long (nullable = true)\n\n+------+----------+-----+------+-----+\n|  Name|Department|State|Salary|Bonus|\n+------+----------+-----+------+-----+\n|  Sona|     Sales|   NY| 90000|10000|\n|  Mona|     Sales|   NY| 86000|20000|\n| Bannu|     Sales|   CA| 81000|23000|\n|Chinnu|   Finance|   CA| 90000|23000|\n| Sunny|   Finance|   CA| 99000|24000|\n|  Raju|   Finance|   NY| 83000|19000|\n|  ravi|   Finance|   NY| 79000|15000|\n| Dhana| Marketing|   CA| 80000|18000|\n|   Ram| Marketing|   NY| 91000|21000|\n+------+----------+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "######## Dropping\n",
    "#1) Drop single Column From DataFrame\n",
    "\n",
    "df1.drop(\"Age\").printSchema()\n",
    "df1.drop(\"Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981337d3-9a66-4b87-b4db-5959f6417073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- State: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- Age: long (nullable = true)\n |-- Bonus: long (nullable = true)\n\n+-----+------+---+-----+\n|State|Salary|Age|Bonus|\n+-----+------+---+-----+\n|   NY| 90000| 34|10000|\n|   NY| 86000| 56|20000|\n|   CA| 81000| 30|23000|\n|   CA| 90000| 24|23000|\n|   CA| 99000| 40|24000|\n|   NY| 83000| 36|19000|\n|   NY| 79000| 53|15000|\n|   CA| 80000| 25|18000|\n|   NY| 91000| 50|21000|\n+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 2) Dropping multiple columns\n",
    "df1.drop(\"Name\",\"Department\").printSchema()\n",
    "df1.drop(\"Name\",\"Department\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3106444c-fd42-4da8-8e2b-f30a4c41285b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+------+---+-----+\n|  Name|Department|State|Salary|Age|Bonus|\n+------+----------+-----+------+---+-----+\n|  ravi|   Finance|   NY| 79000| 53|15000|\n| Dhana| Marketing|   CA| 80000| 25|18000|\n| Bannu|     Sales|   CA| 81000| 30|23000|\n|  Raju|   Finance|   NY| 83000| 36|19000|\n|  Mona|     Sales|   NY| 86000| 56|20000|\n|  Sona|     Sales|   NY| 90000| 34|10000|\n|Chinnu|   Finance|   CA| 90000| 24|23000|\n|   Ram| Marketing|   NY| 91000| 50|21000|\n| Sunny|   Finance|   CA| 99000| 40|24000|\n+------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "############# Sorting\n",
    "# sort() or orderBy() function of PySpark DataFrame to sort DataFrame by ascending or descending order \n",
    "# based on single or multiple columns\n",
    "\n",
    "# 1) sort()\n",
    "\n",
    "# i) sorting one column in ascending order\n",
    "df1.sort(\"Salary\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35a786d-fe0f-4915-8722-a5c17f412ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+------+---+-----+\n|  Name|Department|State|Salary|Age|Bonus|\n+------+----------+-----+------+---+-----+\n| Sunny|   Finance|   CA| 99000| 40|24000|\n|   Ram| Marketing|   NY| 91000| 50|21000|\n|  Sona|     Sales|   NY| 90000| 34|10000|\n|Chinnu|   Finance|   CA| 90000| 24|23000|\n|  Mona|     Sales|   NY| 86000| 56|20000|\n|  Raju|   Finance|   NY| 83000| 36|19000|\n| Bannu|     Sales|   CA| 81000| 30|23000|\n| Dhana| Marketing|   CA| 80000| 25|18000|\n|  ravi|   Finance|   NY| 79000| 53|15000|\n+------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# ii) sorting one column in descending order\n",
    "\n",
    "df1.sort(\"Salary\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2104bb6-4047-44a5-9693-44cfebd0a838",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+------+---+-----+\n|  Name|Department|State|Salary|Age|Bonus|\n+------+----------+-----+------+---+-----+\n|  ravi|   Finance|   NY| 79000| 53|15000|\n|  Raju|   Finance|   NY| 83000| 36|19000|\n|Chinnu|   Finance|   CA| 90000| 24|23000|\n| Sunny|   Finance|   CA| 99000| 40|24000|\n| Dhana| Marketing|   CA| 80000| 25|18000|\n|   Ram| Marketing|   NY| 91000| 50|21000|\n| Bannu|     Sales|   CA| 81000| 30|23000|\n|  Mona|     Sales|   NY| 86000| 56|20000|\n|  Sona|     Sales|   NY| 90000| 34|10000|\n+------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# multiple columns using orderBy()\n",
    "\n",
    "df1.orderBy(\"Department\",\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2a7288-75d0-4ec5-b6e9-582a061ec487",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|Department|sum(Salary)|\n+----------+-----------+\n|     Sales|     257000|\n|   Finance|     351000|\n| Marketing|     171000|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# group by and aggregations\n",
    "# 1) sum of salary department wise\n",
    "df1.groupBy(\"Department\").sum(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecc0aa5-2e04-4529-9479-2b5b2e4aa884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n|Department|count|\n+----------+-----+\n|     Sales|    3|\n|   Finance|    4|\n| Marketing|    2|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 2) number of employees dept wise\n",
    "df1.groupBy(\"Department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e20e56-867f-4f85-802e-defb916c8ba4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|Department|min(Salary)|\n+----------+-----------+\n|     Sales|      81000|\n|   Finance|      79000|\n| Marketing|      80000|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3) Minimum salary for each department\n",
    "df1.groupBy(\"Department\").min(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03034afa-517e-4e35-bc25-79469413f180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|Department|max(Salary)|\n+----------+-----------+\n|     Sales|      90000|\n|   Finance|      99000|\n| Marketing|      91000|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4) Maximum salary of each dept\n",
    "df1.groupBy(\"Department\").max(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315f721e-cd92-46b0-beed-740a396b53f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n|Department|      avg(Salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5) AVerage salary of each dept\n",
    "df1.groupBy(\"Department\").avg(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1761bcc1-5616-42ba-9b98-9615ca84b49d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n|Department|      avg(Salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6) Mean of salary dept wise\n",
    "df1.groupBy(\"Department\").mean(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ea1ca0-12d3-4013-9549-1a4506cc5db0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n|Department|State|sum(Salary)|sum(Bonus)|\n+----------+-----+-----------+----------+\n|     Sales|   NY|     176000|     30000|\n|     Sales|   CA|      81000|     23000|\n|   Finance|   CA|     189000|     47000|\n|   Finance|   NY|     162000|     34000|\n| Marketing|   NY|      91000|     21000|\n| Marketing|   CA|      80000|     18000|\n+----------+-----+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7) group by multiple columns\n",
    "# group by department,state and find sum of salary and bonus\n",
    "df1.groupBy(\"Department\",\"State\").sum(\"Salary\",\"Bonus\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ebc5fe-8811-4ed7-b865-30cb1b645e9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+----------+----------+-----------------+-----------+\n|Department|sum_salary|       avg_salary|Min_salary|Max_Salary|      Mean Salary|Total count|\n+----------+----------+-----------------+----------+----------+-----------------+-----------+\n|     Sales|    257000|85666.66666666667|     81000|     90000|85666.66666666667|          3|\n|   Finance|    351000|          87750.0|     79000|     99000|          87750.0|          4|\n| Marketing|    171000|          85500.0|     80000|     91000|          85500.0|          2|\n+----------+----------+-----------------+----------+----------+-----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#8)  Running more aggregates at a time\n",
    "from pyspark.sql.functions import sum,avg,min,max,mean,count\n",
    "df1.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"sum_salary\"),\n",
    "            avg(\"Salary\").alias(\"avg_salary\"),\n",
    "                min(\"Salary\").alias(\"Min_salary\"),\n",
    "            max(\"Salary\").alias(\"Max_Salary\"),\n",
    "            mean(\"Salary\").alias(\"Mean Salary\"),\n",
    "            count(\"*\").alias(\"Total count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2f1ed4-4368-4145-ad5d-0d5663fee7f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n| ID|  Name|  Company|\n+---+------+---------+\n|  1|  Sona|      TCS|\n|  2|  Mona|  Infosys|\n|  3| Bannu| Delloite|\n|  4| Sunny|      TCS|\n|  5|Chinnu|Accenture|\n+---+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# joins\n",
    "\n",
    "# Creating a datframe for Employee Personal Details\n",
    "data=[(\"1\",\"Sona\",\"TCS\"),\n",
    "      (\"2\",\"Mona\",\"Infosys\"),\n",
    "      (\"3\",\"Bannu\",\"Delloite\"),\n",
    "     (\"4\",\"Sunny\",\"TCS\"),\n",
    "     (\"5\",\"Chinnu\",\"Accenture\")]\n",
    "cols=[\"ID\",\"Name\",\"Company\"]\n",
    "df1=spark.createDataFrame(data,cols)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99d1ed4-06c5-4ad0-89c5-1aa26b1c0055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n| ID|Salary|Department|\n+---+------+----------+\n|  1| 45000|        HR|\n|  2| 90000|   Manager|\n|  6| 78000|        IT|\n|  5| 50000|     Sales|\n+---+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe of employee salary details\n",
    "data=[(\"1\",45000,\"HR\"),\n",
    "     (\"2\",90000,\"Manager\"),\n",
    "     (\"6\",78000,\"IT\"),\n",
    "     (\"5\",50000,\"Sales\")]\n",
    "\n",
    "cols=[\"ID\",\"Salary\",\"Department\"]\n",
    "df2=spark.createDataFrame(data=data,schema=cols)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aca596e-94a2-44a9-9aaa-9705fa529297",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+------+----------+\n| ID|  Name|  Company| ID|Salary|Department|\n+---+------+---------+---+------+----------+\n|  1|  Sona|      TCS|  1| 45000|        HR|\n|  2|  Mona|  Infosys|  2| 90000|   Manager|\n|  5|Chinnu|Accenture|  5| 50000|     Sales|\n+---+------+---------+---+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1) Inner join\n",
    "df1.join(df2,df1.ID==df2.ID,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938947be-b80c-4423-b3c6-4c97e5d3a24a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---------+----+------+----------+\n|  ID|  Name|  Company|  ID|Salary|Department|\n+----+------+---------+----+------+----------+\n|   1|  Sona|      TCS|   1| 45000|        HR|\n|   2|  Mona|  Infosys|   2| 90000|   Manager|\n|   3| Bannu| Delloite|NULL|  NULL|      NULL|\n|   4| Sunny|      TCS|NULL|  NULL|      NULL|\n|   5|Chinnu|Accenture|   5| 50000|     Sales|\n|NULL|  NULL|     NULL|   6| 78000|        IT|\n+----+------+---------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2) outer join\n",
    "df1.join(df2,df1.ID==df2.ID,\"outer\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17aec7e8-3ccc-48ab-853e-8bb5651eee4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+----+------+----------+\n| ID|  Name|  Company|  ID|Salary|Department|\n+---+------+---------+----+------+----------+\n|  1|  Sona|      TCS|   1| 45000|        HR|\n|  2|  Mona|  Infosys|   2| 90000|   Manager|\n|  3| Bannu| Delloite|NULL|  NULL|      NULL|\n|  4| Sunny|      TCS|NULL|  NULL|      NULL|\n|  5|Chinnu|Accenture|   5| 50000|     Sales|\n+---+------+---------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3) left join\n",
    "df1.join(df2,df1.ID==df2.ID,\"leftouter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b304074a-e064-447a-93ea-c1594f59dd34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---------+---+------+----------+\n|  ID|  Name|  Company| ID|Salary|Department|\n+----+------+---------+---+------+----------+\n|   1|  Sona|      TCS|  1| 45000|        HR|\n|   2|  Mona|  Infosys|  2| 90000|   Manager|\n|NULL|  NULL|     NULL|  6| 78000|        IT|\n|   5|Chinnu|Accenture|  5| 50000|     Sales|\n+----+------+---------+---+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4) Right join\n",
    "df1.join(df2,df1.ID==df2.ID,\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298b0140-d85d-47c6-92fe-ea3b312219e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n| ID|  Name|  Company|\n+---+------+---------+\n|  1|  Sona|      TCS|\n|  2|  Mona|  Infosys|\n|  5|Chinnu|Accenture|\n+---+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5) left semi\n",
    "df1.join(df2,df1.ID==df2.ID,\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95be06c9-ad16-4fac-843f-9edacb9e611f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+\n| ID| Name| Company|\n+---+-----+--------+\n|  3|Bannu|Delloite|\n|  4|Sunny|     TCS|\n+---+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6) left anti\n",
    "df1.join(df2,df1.ID==df2.ID,\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8930aa-5b94-4847-a897-55dbec29f9a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question-2\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkCreateTableExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e723415f-e504-4732-a9bb-0454fe1222f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS db;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f77ac3c-22bc-4664-9f27-9704796d7714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a table Employee3\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS Employee3(emp_id Int, emp_name String,Salary INT,age INT,city String)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ef9afd-bc95-43c2-b259-3bf741516ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"INSERT INTO Employee3 VALUES (1,'Sona',80000,22,'Hyderabad')\")\n",
    "spark.sql(\"insert into employee3 values(2,'Sunny',70000,23,'Pune'),(3,'Mona',90000,19,'Hyderabad'),(7,'Chinnu',45000,25,'Mumbai'),(10,'Bannu',12,27,'Pune'),(6,'Raju',70000,45,'Mumbai'),(18,'Dhana',89999,34,'Kolkata')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41093c1-e2bc-4934-a313-c1f55bf7abf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     2|   Sunny| 70000| 23|     Pune|\n|     3|    Mona| 90000| 19|Hyderabad|\n|     7|  Chinnu| 45000| 25|   Mumbai|\n|    10|   Bannu|    12| 27|     Pune|\n|     6|    Raju| 70000| 45|   Mumbai|\n|    18|   Dhana| 89999| 34|  Kolkata|\n|     1|    Sona| 80000| 22|Hyderabad|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select * from employee3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477daaab-a8b6-41df-bf1e-7b19d234ec3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joins\n",
    "# create another table department\n",
    "spark.sql(\"create table if not exists Department1(dept_id int,emp_id int,dept_name string)\")\n",
    "\n",
    "# insert records into department table\n",
    "spark.sql(\"insert into Department1 values(100,1,'IT'),(102,6,'HR'),(103,13,'Manager'),(109,7,'Developer'),(110,10,'Tester')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14631798-c246-48d8-854a-7adda0f68ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n|dept_id|emp_id|dept_name|\n+-------+------+---------+\n|    100|     1|       IT|\n|    102|     6|       HR|\n|    103|    13|  Manager|\n|    109|     7|Developer|\n|    110|    10|   Tester|\n+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select * from department1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed2a4b6-dbc9-47dc-8287-394220afb149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+-------+------+---------+\n|emp_id|emp_name|Salary|age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+---+---------+-------+------+---------+\n|     7|  Chinnu| 45000| 25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12| 27|     Pune|    110|    10|   Tester|\n|     6|    Raju| 70000| 45|   Mumbai|    102|     6|       HR|\n|     1|    Sona| 80000| 22|Hyderabad|    100|     1|       IT|\n+------+--------+------+---+---------+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Inner join\n",
    "spark.sql(\"select * from employee3  inner join  department1 on employee3.emp_id=department1.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2898ad34-2228-4a1b-8627-0e794ff403b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----+---------+-------+------+---------+\n|emp_id|emp_name|Salary| age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+----+---------+-------+------+---------+\n|     1|    Sona| 80000|  22|Hyderabad|    100|     1|       IT|\n|     2|   Sunny| 70000|  23|     Pune|   NULL|  NULL|     NULL|\n|     3|    Mona| 90000|  19|Hyderabad|   NULL|  NULL|     NULL|\n|     6|    Raju| 70000|  45|   Mumbai|    102|     6|       HR|\n|     7|  Chinnu| 45000|  25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12|  27|     Pune|    110|    10|   Tester|\n|  NULL|    NULL|  NULL|NULL|     NULL|    103|    13|  Manager|\n|    18|   Dhana| 89999|  34|  Kolkata|   NULL|  NULL|     NULL|\n+------+--------+------+----+---------+-------+------+---------+\n\n+------+--------+------+----+---------+-------+------+---------+\n|emp_id|emp_name|Salary| age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+----+---------+-------+------+---------+\n|     1|    Sona| 80000|  22|Hyderabad|    100|     1|       IT|\n|     2|   Sunny| 70000|  23|     Pune|   NULL|  NULL|     NULL|\n|     3|    Mona| 90000|  19|Hyderabad|   NULL|  NULL|     NULL|\n|     6|    Raju| 70000|  45|   Mumbai|    102|     6|       HR|\n|     7|  Chinnu| 45000|  25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12|  27|     Pune|    110|    10|   Tester|\n|  NULL|    NULL|  NULL|NULL|     NULL|    103|    13|  Manager|\n|    18|   Dhana| 89999|  34|  Kolkata|   NULL|  NULL|     NULL|\n+------+--------+------+----+---------+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Outer join\n",
    "# Full outer join\n",
    "spark.sql(\"select * from employee3  full outer join  department1 on employee3.emp_id=department1.emp_id\").show()\n",
    "\n",
    "spark.sql(\"select * from employee3  full join  department1 on employee3.emp_id=department1.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9cd8d54-a414-4420-891b-0352d7d2d3cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+-------+------+---------+\n|emp_id|emp_name|Salary|age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+---+---------+-------+------+---------+\n|     2|   Sunny| 70000| 23|     Pune|   NULL|  NULL|     NULL|\n|     3|    Mona| 90000| 19|Hyderabad|   NULL|  NULL|     NULL|\n|     7|  Chinnu| 45000| 25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12| 27|     Pune|    110|    10|   Tester|\n|     6|    Raju| 70000| 45|   Mumbai|    102|     6|       HR|\n|    18|   Dhana| 89999| 34|  Kolkata|   NULL|  NULL|     NULL|\n|     1|    Sona| 80000| 22|Hyderabad|    100|     1|       IT|\n+------+--------+------+---+---------+-------+------+---------+\n\n+------+--------+------+---+---------+-------+------+---------+\n|emp_id|emp_name|Salary|age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+---+---------+-------+------+---------+\n|     2|   Sunny| 70000| 23|     Pune|   NULL|  NULL|     NULL|\n|     3|    Mona| 90000| 19|Hyderabad|   NULL|  NULL|     NULL|\n|     7|  Chinnu| 45000| 25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12| 27|     Pune|    110|    10|   Tester|\n|     6|    Raju| 70000| 45|   Mumbai|    102|     6|       HR|\n|    18|   Dhana| 89999| 34|  Kolkata|   NULL|  NULL|     NULL|\n|     1|    Sona| 80000| 22|Hyderabad|    100|     1|       IT|\n+------+--------+------+---+---------+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left join\n",
    "# Left outer join\n",
    "spark.sql(\"select * from employee3  left join  department1 on employee3.emp_id=department1.emp_id\").show()\n",
    "\n",
    "spark.sql(\"select * from employee3  left outer join  department1 on employee3.emp_id=department1.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c68d6c72-7600-42ba-9aaf-a9a9fa5a5807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----+---------+-------+------+---------+\n|emp_id|emp_name|Salary| age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+----+---------+-------+------+---------+\n|     1|    Sona| 80000|  22|Hyderabad|    100|     1|       IT|\n|     6|    Raju| 70000|  45|   Mumbai|    102|     6|       HR|\n|  NULL|    NULL|  NULL|NULL|     NULL|    103|    13|  Manager|\n|     7|  Chinnu| 45000|  25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12|  27|     Pune|    110|    10|   Tester|\n+------+--------+------+----+---------+-------+------+---------+\n\n+------+--------+------+----+---------+-------+------+---------+\n|emp_id|emp_name|Salary| age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+----+---------+-------+------+---------+\n|     1|    Sona| 80000|  22|Hyderabad|    100|     1|       IT|\n|     6|    Raju| 70000|  45|   Mumbai|    102|     6|       HR|\n|  NULL|    NULL|  NULL|NULL|     NULL|    103|    13|  Manager|\n|     7|  Chinnu| 45000|  25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12|  27|     Pune|    110|    10|   Tester|\n+------+--------+------+----+---------+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Right join\n",
    "# Right outer join\n",
    "spark.sql(\"select * from employee3  right outer join  department1 on employee3.emp_id=department1.emp_id\").show()\n",
    "\n",
    "spark.sql(\"select * from employee3  right join  department1 on employee3.emp_id=department1.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc2b843-abfb-4eee-b727-9b098b2467fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     7|  Chinnu| 45000| 25|   Mumbai|\n|    10|   Bannu|    12| 27|     Pune|\n|     6|    Raju| 70000| 45|   Mumbai|\n|     1|    Sona| 80000| 22|Hyderabad|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left semi join\n",
    "spark.sql(\"select * from employee3  left semi join  department1 on employee3.emp_id=department1.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feecaa88-eedc-4003-afd4-9ac6d75b064b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     2|   Sunny| 70000| 23|     Pune|\n|     3|    Mona| 90000| 19|Hyderabad|\n|    18|   Dhana| 89999| 34|  Kolkata|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left anti join\n",
    "spark.sql(\"select * from employee3  left anti join  department1 on employee3.emp_id=department1.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63a8edc-30d9-468a-950e-8d7f17ec9ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b\n0  1  4\n1  2  5\n2  3  6\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a   b\n",
       "0  11  14\n",
       "1  12  15\n",
       "2  13  16"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) Applying Functions in pandas DF\n",
    "# 1) transform()\n",
    "\n",
    "import pandas as pd\n",
    "psdf = pd.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "print(psdf)\n",
    "\n",
    "# Adding 10 to each element\n",
    "\n",
    "def pandas_plus(count):\n",
    "     return count + 10  # should always return the same length as input.\n",
    "    \n",
    "psdf.transform(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21de164e-7808-4516-8694-ab2161570988",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  5\n",
       "2  3  7"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) apply\n",
    "\n",
    "psdf = pd.DataFrame({'a': [1,2,3], 'b':[5,6,7]})\n",
    "def pandas_plus(x):\n",
    "    return x[x % 2 == 1]  # allows an arbitrary length\n",
    "\n",
    "psdf.apply(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4eafada-0aa8-4478-912a-400828f4aa2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n       Fee  Discount\n0  20000.0      1000\n1  25000.0      2500\n2  30000.0      1500\n3  22000.0      1200\n4      NaN      3000\nFee         45000.0\nDiscount     3500.0\ndtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "import pyspark.pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "technologies = ({\n",
    "    'Fee' :[20000,25000,30000,22000,np.NaN],\n",
    "    'Discount':[1000,2500,1500,1200,3000]\n",
    "               })\n",
    "\n",
    "psdf = pd.DataFrame(technologies)\n",
    "print(type(psdf))\n",
    "print(psdf)\n",
    "\n",
    "\n",
    "def add(data):\n",
    "   return data[0]+data[1]\n",
    "  \n",
    "addDF = psdf.apply(add)\n",
    "print(addDF)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark coding challenge",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
