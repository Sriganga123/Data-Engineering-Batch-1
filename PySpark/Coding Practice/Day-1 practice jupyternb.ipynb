{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9641341-cae3-4fe3-925c-7eea82dcfd1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Creating Spark Session-Method-1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m spark\u001b[38;5;241m=\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[1]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating Spark Session\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      4\u001b[0m spark\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Creating Spark Session-Method-1\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"Creating Spark Session\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c7be83-845f-4f5a-8e96-086c301d6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark Version\"+spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a4e3f0-71e2-4063-b2f7-9817d1dd9747",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Creating Spark Session method-2 using newSession\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark2\u001b[38;5;241m=\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mnewSession\n\u001b[0;32m      3\u001b[0m spark2\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating Spark Session method-2 using newSession\n",
    "spark2=SparkSession.newSession\n",
    "spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0984dde-25f1-4a3a-be3d-8c331a0fd65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function SparkSession.newSession at 0x000002625A7984A0>\n"
     ]
    }
   ],
   "source": [
    "print(spark2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e1c356-0b11-4913-9ed8-ebaeda688e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x000002624BEDDB20>>\n"
     ]
    }
   ],
   "source": [
    "# Get Existing SparkSession\n",
    "spark3 = SparkSession.builder.getOrCreate\n",
    "print(spark3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237cf68c-59d1-43a4-b827-3f689ff59d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x000002624BEDDB20>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "795a4f39-03ae-4210-8a0e-1c4177ebce23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create RDD from parallelize    \n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"Creating RDD from paralleize\").getOrCreate()\n",
    "spark\n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99557ff1-61d0-479a-a2b4-7938c4ffd246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-HPGQI4S:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Creating Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2625a90c950>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create RDD from external Data source\n",
    "#Create RDD using sparkContext.textFile()\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Creating RDD from external Source file\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87ef4d55-ac9c-499e-98cc-c9afd81f9edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Datatypes',\n",
       " 'print(\"Datatypes\")',\n",
       " '#int',\n",
       " 'print(type(12))',\n",
       " '#float',\n",
       " 'print(type(18.9))',\n",
       " '#complex',\n",
       " 'print(type(2+4j))',\n",
       " '#List',\n",
       " \"l=[12,11,90.89,'apple']\",\n",
       " 'print(type(l))',\n",
       " '#Tuple',\n",
       " \"t=(12,90,34.33,'tuple')\",\n",
       " 'print(type(t))',\n",
       " '#Set',\n",
       " 's={1,2,3,4,5}',\n",
       " 'print(type(s))',\n",
       " '#Dictionary',\n",
       " \"d={1:'a',2:'b',3:'c'}\",\n",
       " 'print(type(d))',\n",
       " '# String',\n",
       " \"str='Hello'\",\n",
       " 'print(type(str))',\n",
       " '#Boolean',\n",
       " 'b=True',\n",
       " 'print(type(b))']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.sparkContext.textFile(\"C:\\\\Users\\\\DELL\\\\Desktop\\\\pyspark\\\\abc.txt\")\n",
    "df\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aae2034-6383-4574-ab88-be13fac62142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/C:/Users/DELL/OneDrive/Desktop/Data Engineering Python/DEPython/app.txt',\n",
       "  'I am Sriganga.I am from Nizamabad.Hello WelcomeHello WelcomeHello WelcomeHello Welcome')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create RDD using sparkContext.wholeTextFiles()\n",
    "#Reads entire file into a RDD as single record.\n",
    "df1=spark.sparkContext.wholeTextFiles(\"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Data Engineering Python\\\\DEPython\\\\app.txt\")\n",
    "df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25f3da5-2605-4287-8048-e3085c050a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-HPGQI4S:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Count</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2201a31f0e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the number of elements\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"Count\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd04c476-5344-4cf9-9647-3b4b488d9ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e31bd1dc-bc04-4901-8c37-78798d8c1bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the number of elements\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local[1]\").appName(\"Count\").getOrCreate()\n",
    "spark\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d31ad-971a-4ebd-954f-6c6a2a1b29e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832a476b-6fde-4e9b-a64d-745a0c5bc6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    }
   ],
   "source": [
    "print(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9cddfd-ac1a-4345-812f-812ed09ab62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
