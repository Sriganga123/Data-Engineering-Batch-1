{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dba1514-65a6-443f-b435-19c863824658",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to data bricks\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to data bricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebdf7baf-ddc7-4805-a36d-f2d50b9e685a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Java', 20000), ('Python', 10000), ('Scala', 30000)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "# define data\n",
    "dataList=[(\"Java\",20000),(\"Python\",10000),(\"Scala\",30000)]\n",
    "#usind parallelize\n",
    "rdd=spark.sparkContext.parallelize(dataList)\n",
    "# To print the data\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f707d149-d638-4809-b485-ab0b8801018e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d51f706-c74a-4ad7-8d50-f1a86466cc73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n|_c0|   _c1|_c2|   _c3|\n+---+------+---+------+\n| ID|  Name|Age|Salary|\n|  1|  Sona| 12| 12000|\n|  2|  Mona| 21| 43000|\n|  3| Bannu| 13| 25000|\n|  4|Chinnu| 23| 10000|\n|  5| sunny| 14| 23000|\n|  6|  raju| 24| 78000|\n|  7|   ram| 15| 45000|\n|  8| lucky| 25| 50000|\n|  9|  Appy| 17| 60000|\n| 10| Lekha| 38| 21890|\n| 11|Vinnni| 11| 78654|\n| 12| pavan| 19| 34567|\n| 13|  Sita| 32| 78901|\n| 14| latha| 33| 56455|\n| 15|  Gita| 28| 25678|\n+---+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Employee Details\").getOrCreate()\n",
    "spark\n",
    "df=spark.read.csv('/FileStore/tables/StudentData/Example_1.csv')\n",
    "df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308b8214-0cda-4e45-b520-ad52429aaa48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: ['Apache Spark is a lightning fast real-time processing framework. It does in-memory computations to analyze data in real-time. It came into picture as Apache Hadoop MapReduce was performing batch processing only and lacked a real-time processing feature. Hence, Apache Spark was introduced as it can perform stream processing in real-time and can also take care of batch processing.']"
     ]
    }
   ],
   "source": [
    "# Using text file\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Data from text file\").getOrCreate()\n",
    "spark\n",
    "df=spark.sparkContext.textFile(\"/FileStore/tables/spark/data1.txt\")\n",
    "df\n",
    "df.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53cbf5c-b562-46c9-adb0-efbc3e541865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark_examples').getOrCreate()\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
    "  ]\n",
    "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e53e5c-dcd3-46b8-91fc-dbd11c1ecec3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "count_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n",
    "print(\"Count\",count_rdd.count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2379681506183288,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
