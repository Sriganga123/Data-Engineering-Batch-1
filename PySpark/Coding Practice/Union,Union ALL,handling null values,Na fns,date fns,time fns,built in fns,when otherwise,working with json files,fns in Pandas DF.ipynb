{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da13993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union and union all\n",
    "import pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6558ef54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-HPGQI4S:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>union and union all</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dc3d61e750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"union and union all\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b105d73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe-1\n",
    "Data = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \n",
    "(\"Michael\",\"Sales\",\"NY\",86000,56,20000), \n",
    "(\"Robert\",\"Sales\",\"CA\",81000,30,23000), \n",
    "(\"Maria\",\"Finance\",\"CA\",90000,24,23000) \n",
    "]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df1=spark.createDataFrame(data=Data,schema=columns)\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2207a6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe-2 having same schema as that of df1\n",
    "Data2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \n",
    "(\"Maria\",\"Finance\",\"CA\",90000,24,23000), \n",
    "(\"Jen\",\"Finance\",\"NY\",79000,53,15000), \n",
    "(\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \n",
    "(\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \n",
    "]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2=spark.createDataFrame(Data2,columns2)\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcd987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) UNION\n",
    "# union() method merges two DataFrames and returns the new DataFrame with all rows from two Dataframes \n",
    "#regardless of duplicate data.\n",
    "\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeec91c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unionall'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2) UNION ALL\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# same as union but it is deprecated so recommended to use union\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df1\u001b[38;5;241m.\u001b[39munionall(df2)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:3123\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[0;32m   3091\u001b[0m \n\u001b[0;32m   3092\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3120\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[0;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m-> 3123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   3124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[0;32m   3125\u001b[0m     )\n\u001b[0;32m   3126\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unionall'"
     ]
    }
   ],
   "source": [
    "# 2) UNION ALL\n",
    "# same as union but it is deprecated so recommended to use union\n",
    "\n",
    "df1.unionall(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5291f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Merge without Duplicates\n",
    "\n",
    "#Since the union() method returns all rows without distinct records, we will use\n",
    "#the distinct() function to return just one record when a duplicate exists.\n",
    "\n",
    "df1.union(df2).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb54b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "| Krish|  31|        10| 30000|\n",
      "|  Sona|  30|         8| 25000|\n",
      "|  Mona|  29|         4| 20000|\n",
      "| Dhana|  24|         3| 20000|\n",
      "|  Raju|  21|         1| 15000|\n",
      "| Bannu|  23|         2| 18000|\n",
      "|Chinnu|NULL|      NULL| 48999|\n",
      "|  NULL|  34|        10| 38000|\n",
      "|  NULL|  36|      NULL|  NULL|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling Null values\n",
    "\n",
    "# Creating a dataframe containing null values\n",
    "df1=spark.read.csv(\"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Data Engineering Python\\\\DEPython\\\\mising.csv\",header=True,inferSchema=True)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9a977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Chinnu|NULL|      NULL| 48999|\n",
      "|  NULL|  36|      NULL|  NULL|\n",
      "+------+----+----------+------+\n",
      "\n",
      "+------+----+----------+------+\n",
      "|  Name| age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Chinnu|NULL|      NULL| 48999|\n",
      "|  NULL|  36|      NULL|  NULL|\n",
      "+------+----+----------+------+\n",
      "\n",
      "+------+----+----------+------+\n",
      "|  Name| age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Chinnu|NULL|      NULL| 48999|\n",
      "|  NULL|  36|      NULL|  NULL|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Filter Rows with NULL Values in DataFrame\n",
    "# use filter() or where()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df1.filter(col(\"Experience\").isNull()).show()\n",
    "df1.filter(df1.Experience.isNull()).show()\n",
    "df1.filter(\"Experience is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6513fe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "|NULL| 36|      NULL|  NULL|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Filter Rows with NULL on Multiple Columns\n",
    "df1.filter(df1.Name.isNull() & df1.Experience.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c218232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Krish| 31|        10| 30000|\n",
      "| Sona| 30|         8| 25000|\n",
      "| Mona| 29|         4| 20000|\n",
      "|Dhana| 24|         3| 20000|\n",
      "| Raju| 21|         1| 15000|\n",
      "|Bannu| 23|         2| 18000|\n",
      "| NULL| 34|        10| 38000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Filter Rows with IS NOT NULL or isNotNull\n",
    "df1.filter(df1.Experience.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4915a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Krish| 31|        10| 30000|\n",
      "| Sona| 30|         8| 25000|\n",
      "| Mona| 29|         4| 20000|\n",
      "|Dhana| 24|         3| 20000|\n",
      "| Raju| 21|         1| 15000|\n",
      "|Bannu| 23|         2| 18000|\n",
      "| NULL| 34|        10| 38000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or it can also be written using  na drop\n",
    "df1.na.drop(subset=[\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de53c894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+------+\n",
      "|  Name|  age|Experience|Salary|\n",
      "+------+-----+----------+------+\n",
      "| Krish|   31|        10| 30000|\n",
      "|  Sona|   30|         8| 25000|\n",
      "|  Mona|   29|         4| 20000|\n",
      "| Dhana|   24|         3| 20000|\n",
      "|  Raju|   21|         1| 15000|\n",
      "| Bannu|   23|         2| 18000|\n",
      "|Chinnu|10000|     10000| 48999|\n",
      "|  NULL|   34|        10| 38000|\n",
      "|  NULL|   36|     10000| 10000|\n",
      "+------+-----+----------+------+\n",
      "\n",
      "+------+-----+----------+------+\n",
      "|  Name|  age|Experience|Salary|\n",
      "+------+-----+----------+------+\n",
      "| Krish|   31|        10| 30000|\n",
      "|  Sona|   30|         8| 25000|\n",
      "|  Mona|   29|         4| 20000|\n",
      "| Dhana|   24|         3| 20000|\n",
      "|  Raju|   21|         1| 15000|\n",
      "| Bannu|   23|         2| 18000|\n",
      "|Chinnu|10000|     10000| 48999|\n",
      "|  NULL|   34|        10| 38000|\n",
      "|  NULL|   36|     10000| 10000|\n",
      "+------+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  fillna() & fill() to replace null values or None values\n",
    "\n",
    "df1.na.fill(value=10000).show()  # fill null values with 10000 of int type\n",
    "df1.fillna(value=10000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4993ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "| Krish| 31|        10| 30000|\n",
      "|  Sona| 30|         8| 25000|\n",
      "|  Mona| 29|         4| 20000|\n",
      "| Dhana| 24|         3| 20000|\n",
      "|  Raju| 21|         1| 15000|\n",
      "| Bannu| 23|         2| 18000|\n",
      "|Chinnu|  0|      NULL| 48999|\n",
      "|  NULL| 34|        10| 38000|\n",
      "|  NULL| 36|      NULL|  NULL|\n",
      "+------+---+----------+------+\n",
      "\n",
      "+------+---+----------+------+\n",
      "|  Name|age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "| Krish| 31|        10| 30000|\n",
      "|  Sona| 30|         8| 25000|\n",
      "|  Mona| 29|         4| 20000|\n",
      "| Dhana| 24|         3| 20000|\n",
      "|  Raju| 21|         1| 15000|\n",
      "| Bannu| 23|         2| 18000|\n",
      "|Chinnu|  0|      NULL| 48999|\n",
      "|  NULL| 34|        10| 38000|\n",
      "|  NULL| 36|      NULL|  NULL|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill all null values with 0 in age column\n",
    "df1.na.fill(value=0,subset=[\"age\"]).show()\n",
    "         # or\n",
    "df1.fillna(value=0,subset=[\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "607ff2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|  Krish|  31|        10| 30000|\n",
      "|   Sona|  30|         8| 25000|\n",
      "|   Mona|  29|         4| 20000|\n",
      "|  Dhana|  24|         3| 20000|\n",
      "|   Raju|  21|         1| 15000|\n",
      "|  Bannu|  23|         2| 18000|\n",
      "| Chinnu|NULL|      NULL| 48999|\n",
      "|UNKNOWN|  34|        10| 38000|\n",
      "|UNKNOWN|  36|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill null values with some string\n",
    "df1.na.fill(value=\"UNKNOWN\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa4870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Krish| 31|        10| 30000|\n",
      "| Sona| 30|         8| 25000|\n",
      "| Mona| 29|         4| 20000|\n",
      "|Dhana| 24|         3| 20000|\n",
      "| Raju| 21|         1| 15000|\n",
      "|Bannu| 23|         2| 18000|\n",
      "+-----+---+----------+------+\n",
      "\n",
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Krish| 31|        10| 30000|\n",
      "| Sona| 30|         8| 25000|\n",
      "| Mona| 29|         4| 20000|\n",
      "|Dhana| 24|         3| 20000|\n",
      "| Raju| 21|         1| 15000|\n",
      "|Bannu| 23|         2| 18000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Rows with NULL Values in Any Columns\n",
    "df1.na.drop().show()\n",
    "df1.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04fc4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "| Krish|  31|        10| 30000|\n",
      "|  Sona|  30|         8| 25000|\n",
      "|  Mona|  29|         4| 20000|\n",
      "| Dhana|  24|         3| 20000|\n",
      "|  Raju|  21|         1| 15000|\n",
      "| Bannu|  23|         2| 18000|\n",
      "|Chinnu|NULL|      NULL| 48999|\n",
      "|  NULL|  34|        10| 38000|\n",
      "|  NULL|  36|      NULL|  NULL|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop Rows with NULL Values on All Columns\n",
    "df1.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3f221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Krish| 31|        10| 30000|\n",
      "| Sona| 30|         8| 25000|\n",
      "| Mona| 29|         4| 20000|\n",
      "|Dhana| 24|         3| 20000|\n",
      "| Raju| 21|         1| 15000|\n",
      "|Bannu| 23|         2| 18000|\n",
      "| NULL| 34|        10| 38000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop Rows with NULL Values on Selected Columns\n",
    "df1.na.drop(subset=[\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863e6719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Krish| 31|        10| 30000|\n",
      "| Sona| 30|         8| 25000|\n",
      "| Mona| 29|         4| 20000|\n",
      "|Dhana| 24|         3| 20000|\n",
      "| Raju| 21|         1| 15000|\n",
      "|Bannu| 23|         2| 18000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using any\n",
    "df1.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "574620d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "| Krish|  31|        10| 30000|\n",
      "|  Sona|  30|         8| 25000|\n",
      "|  Mona|  29|         4| 20000|\n",
      "| Dhana|  24|         3| 20000|\n",
      "|  Raju|  21|         1| 15000|\n",
      "| Bannu|  23|         2| 18000|\n",
      "|Chinnu|NULL|      NULL| 48999|\n",
      "|  NULL|  34|        10| 38000|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using thresh\n",
    "#drop rows where there are less than 2 non-null values in a row\n",
    "# Minimum of two non null values should be present in a row if less than 2 non null values r present it drops off null value.\n",
    "df1.na.drop(how=\"any\",thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc6dd789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Date functions\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f7855b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Today's date|\n",
      "+------------+\n",
      "|  2024-02-10|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) current date\n",
    "df.select(current_date().alias(\"Today's date\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2678188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|     input|Date format changed|\n",
      "+----------+-------------------+\n",
      "|2020-02-01|         02-01-2020|\n",
      "|2019-03-01|         03-01-2019|\n",
      "|2021-03-01|         03-01-2021|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) date_format()\n",
    "df.select(col(\"input\"),date_format(col(\"input\"),\"MM-dd-yyyy\").alias(\"Date format changed\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a645134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     input|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) to_date() --- converts string to datatype\n",
    "df1=df.select(col(\"input\"),to_date(col(\"input\"),\"yyyy-MM-dd\").alias(\"to_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e7ef7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------+\n",
      "|     input|datedifference b/w input and current date|\n",
      "+----------+-----------------------------------------+\n",
      "|2020-02-01|                                     1470|\n",
      "|2019-03-01|                                     1807|\n",
      "|2021-03-01|                                     1076|\n",
      "+----------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) datediff\n",
    "df.select(col(\"input\"), datediff(current_date(),col(\"input\")).alias(\"datedifference b/w input and current date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95f97443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|     input|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   48.29032258|\n",
      "|2019-03-01|   59.29032258|\n",
      "|2021-03-01|   35.29032258|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) months_between() ---- months between input and current date\n",
    "df.select(col(\"input\"), months_between(current_date(),col(\"input\")).alias(\"months_between\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2e4a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6) add_months() ,  7) date_add(), 8) date_sub()\n",
    "df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95d02461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9) year(), 10) month(), 11) month(), 12) next_day(), 13)weekofyear()\n",
    "\n",
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9401ee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|     input|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14) dayofweek(), 15) dayofmonth(), 16) dayofyear()\n",
    "\n",
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba38d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Timestamp or Time functions\n",
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "421215e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|current_timestamp         |\n",
      "+--------------------------+\n",
      "|2024-02-10 09:30:00.840099|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) current_timestamp()\n",
    "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  ).show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beb987ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) to_timestamp()\n",
    "#Converts string timestamp to Timestamp type format.\n",
    "\n",
    "df2.select(col(\"input\"), to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4feaac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) hour(), 4) Minute() and 5) second()\n",
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e87179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built in functions\n",
    "# a) String functions\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c75ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8afbcbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+---------------+------------------+------------+\n",
      "|name           |languagesAtSchool |currentState|\n",
      "+---------------+------------------+------------+\n",
      "|James Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael Rose   |[Spark, Java, C++]|NJ          |\n",
      "|Robert Williams|[CSharp, VB]      |NV          |\n",
      "+---------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) concat_ws\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael Rose\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c415001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------+---------------------------+\n",
      "|name           |languagesAtSchool |currentState|Student Data               |\n",
      "+---------------+------------------+------------+---------------------------+\n",
      "|James Smith    |[Java, Scala, C++]|CA          |James Smith-Java-Scala-C++ |\n",
      "|Michael Rose   |[Spark, Java, C++]|NJ          |Michael Rose-Spark-Java-C++|\n",
      "|Robert Williams|[CSharp, VB]      |NV          |Robert Williams-CSharp-VB  |\n",
      "+---------------+------------------+------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.withColumn(\"Student Data\",concat_ws(\"-\",df.name,df.languagesAtSchool)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8de36d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------+------+\n",
      "|           name| languagesAtSchool|currentState|Length|\n",
      "+---------------+------------------+------------+------+\n",
      "|    James Smith|[Java, Scala, C++]|          CA|    11|\n",
      "|   Michael Rose|[Spark, Java, C++]|          NJ|    12|\n",
      "|Robert Williams|      [CSharp, VB]|          NV|    15|\n",
      "+---------------+------------------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) length\n",
    "\n",
    "df.withColumn(\"Length\",length(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584f31df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|LOWER CASE|\n",
      "+----------+\n",
      "|        ca|\n",
      "|        nj|\n",
      "|        nv|\n",
      "+----------+\n",
      "\n",
      "+---------------+\n",
      "|     UPPER CASE|\n",
      "+---------------+\n",
      "|    JAMES SMITH|\n",
      "|   MICHAEL ROSE|\n",
      "|ROBERT WILLIAMS|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) lower()\n",
    "df.select(lower(df.currentState).alias(\"LOWER CASE\")).show()\n",
    "\n",
    "# 4) upper()\n",
    "df.select(upper(\"name\").alias(\"UPPER CASE\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85acc30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+------------+\n",
      "|           name|languagesAtSchool|currentState|\n",
      "+---------------+-----------------+------------+\n",
      "|Robert Williams|     [CSharp, VB]|          NV|\n",
      "+---------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) startswith\n",
    "df.filter(col(\"name\").startswith('R')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3de8932f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+------------+\n",
      "|           name|languagesAtSchool|currentState|\n",
      "+---------------+-----------------+------------+\n",
      "|Robert Williams|     [CSharp, VB]|          NV|\n",
      "+---------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6) endswith\n",
    "df.filter(col(\"currentState\").endswith(\"V\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7053c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|substring(name, 5, 4)|\n",
      "+---------------------+\n",
      "|                 s Sm|\n",
      "|                 ael |\n",
      "|                 rt W|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7) substring\n",
    "df.select(substring(\"name\",5,4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "128e466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|L-PADDED|\n",
      "+--------+\n",
      "|   000CA|\n",
      "|   000NJ|\n",
      "|   000NV|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8) lpad\n",
    "df.select(lpad('currentState',5,'0').alias(\"L-PADDED\")).show()  # pad 0 s left side so that length becomes 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d56efbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|R-PADDED|\n",
      "+--------+\n",
      "|   CAxxx|\n",
      "|   NJxxx|\n",
      "|   NVxxx|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9) rpad\n",
    "df.select(rpad('currentState',5,'x').alias(\"R-PADDED\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6923c0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|     Final name|\n",
      "+---------------+\n",
      "|    James Smith|\n",
      "|   Michael Rose|\n",
      "|Robert Williams|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10) trim\n",
    "df.select(trim(df.name).alias(\"Final name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ea88f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|     Final name|\n",
      "+---------------+\n",
      "|    James Smith|\n",
      "|   Michael Rose|\n",
      "|Robert Williams|\n",
      "+---------------+\n",
      "\n",
      "+---------------+\n",
      "|     Final name|\n",
      "+---------------+\n",
      "|    James Smith|\n",
      "|   Michael Rose|\n",
      "|Robert Williams|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11) ltrim (removes leading spaces)\n",
    "df.select(ltrim(df.name).alias(\"Final name\")).show()\n",
    "\n",
    "# 12) rtrim (removes trailing spaces)\n",
    "df.select(rtrim(df.name).alias(\"Final name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95385ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       After split|\n",
      "+------------------+\n",
      "|    [James, Smith]|\n",
      "|   [Michael, Rose]|\n",
      "|[Robert, Williams]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13) split\n",
    "df.select(split(df.name,' ').alias(\"After split\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6646abf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Square root|\n",
      "+-----------+\n",
      "|        4.0|\n",
      "+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Math functions\n",
    "\n",
    "#1) sqrt\n",
    "df.select(sqrt(lit(16)).alias(\"Square root\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40ff51b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Absolute|\n",
      "+--------+\n",
      "|     111|\n",
      "+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) abs\n",
    "df.select(abs(lit(-111)).alias(\"Absolute\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f92ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Ceil|\n",
      "+----+\n",
      "|  13|\n",
      "+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) ceil()\n",
    "df.select(ceil(lit(12.8)).alias(\"Ceil\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bbe97c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Floor|\n",
      "+-----+\n",
      "|   12|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) floor()\n",
    "df.select(floor(lit(12.8)).alias(\"Floor\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62f34127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Factorial|\n",
      "+---------+\n",
      "|      720|\n",
      "+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) factorial()\n",
    "df.select(factorial(lit(6)).alias(\"Factorial\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94f8b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Log 10 value|\n",
      "+------------+\n",
      "|         3.0|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6) log10()\n",
    "df.select(log10(lit(1000)).alias(\"Log 10 value\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20bf8214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Power of a number|\n",
      "+-----------------+\n",
      "|             16.0|\n",
      "+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7) pow(col1,col2)\n",
    "df.select(pow(lit(4),lit(2)).alias(\"Power of a number\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c99fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Binary of number|\n",
      "+----------------+\n",
      "|            1100|\n",
      "+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8) bin(col)\n",
    "df.select(bin(lit(12)).alias(\"Binary of number\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "efef362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Cube root|\n",
      "+---------+\n",
      "|      3.0|\n",
      "+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9) cbrt(col)\n",
    "df.select(cbrt(lit(27)).alias(\"Cube root\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94beb714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------+----------+\n",
      "|           name| languagesAtSchool|currentState| New State|\n",
      "+---------------+------------------+------------+----------+\n",
      "|    James Smith|[Java, Scala, C++]|          CA|  Carolina|\n",
      "|   Michael Rose|[Spark, Java, C++]|          NJ|New Jersey|\n",
      "|Robert Williams|      [CSharp, VB]|          NV|        NV|\n",
      "+---------------+------------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when or otherwise\n",
    "\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael Rose\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df.withColumn(\"New State\", when(df.currentState =='CA','Carolina')\\\n",
    "                          .when(df.currentState =='NJ','New Jersey')\\\n",
    "                          .when(df.currentState == 'NZ','New Venezula')\\\n",
    "                          .otherwise(df.currentState)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fc8fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|           name| new_State|\n",
      "+---------------+----------+\n",
      "|    James Smith|  Carolina|\n",
      "|   Michael Rose|New Jersey|\n",
      "|Robert Williams|        NV|\n",
      "+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using spark sql\n",
    "df.createOrReplaceTempView(\"Person\")\n",
    "spark.sql(\"select name, CASE WHEN currentState = 'CA' THEN 'Carolina' \" + \n",
    "               \"WHEN currentState = 'NJ' THEN 'New Jersey' WHEN currentState IS NULL THEN ''\" +\n",
    "              \"ELSE currentState END as new_State from Person\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b67d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb69ff84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[City: string, Country: string, Decommisioned: boolean, EstimatedPopulation: bigint, Lat: double, Location: string, LocationText: string, LocationType: string, Long: double, Notes: string, RecordNumber: bigint, State: string, TaxReturnsFiled: bigint, TotalWages: bigint, WorldRegion: string, Xaxis: double, Yaxis: double, Zaxis: double, ZipCodeType: string, Zipcode: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working with JSON files\n",
    "\n",
    "#Read JSON file into DataFrame\n",
    "\n",
    "# 1) using read.json\n",
    "df=spark.read.json(\"C:\\\\Users\\\\DELL\\\\Downloads\\\\zipcodes.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81437795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               NULL|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         NULL|           1|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               NULL|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         NULL|           2|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               NULL|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         NULL|       61391|   TX|           NULL|      NULL|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         NULL|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         NULL|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               NULL|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         NULL|           4|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         NULL|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         NULL|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               NULL|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         NULL|       49346|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         NULL|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               NULL|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         NULL|       49348|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               NULL|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         NULL|           3|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               NULL|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         NULL|       54354|   AL|           NULL|      NULL|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         NULL|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         NULL|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         NULL|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         NULL|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b74adc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[City: string, Country: string, Decommisioned: boolean, EstimatedPopulation: bigint, Lat: double, Location: string, LocationText: string, LocationType: string, Long: double, Notes: string, RecordNumber: bigint, State: string, TaxReturnsFiled: bigint, TotalWages: bigint, WorldRegion: string, Xaxis: double, Yaxis: double, Zaxis: double, ZipCodeType: string, Zipcode: bigint]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) using format\n",
    "\n",
    "df=spark.read.format(\"json\").load(\"C:\\\\Users\\\\DELL\\\\Downloads\\\\zipcodes.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b93959ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               NULL|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         NULL|           1|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               NULL|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         NULL|           2|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               NULL|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         NULL|       61391|   TX|           NULL|      NULL|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         NULL|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         NULL|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               NULL|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         NULL|           4|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         NULL|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         NULL|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               NULL|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         NULL|       49346|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         NULL|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               NULL|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         NULL|       49348|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               NULL|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         NULL|           3|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               NULL|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         NULL|       54354|   AL|           NULL|      NULL|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         NULL|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         NULL|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         NULL|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         NULL|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e91d240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-----+--------------------+---------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|City       |Country|Decommisioned|Lat  |Location            |LocationText   |LocationType  |Long  |RecordNumber|State|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-----------+-------+-------------+-----+--------------------+---------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|PARC PARQUE|US     |false        |17.96|NA-US-PR-PARC PARQUE|Parc Parque, PR|NOT ACCEPTABLE|-66.22|1           |PR   |NA         |0.38 |-0.87|0.3  |STANDARD   |704    |\n",
      "+-----------+-------+-------------+-----+--------------------+---------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Read JSON file from multiline\n",
    "# Using read.option(\"multiline\",\"true\")\n",
    "\n",
    "#use multiline option to read JSON files scattered across multiple lines. By default multiline option, is set to false.\n",
    "\n",
    "\n",
    "df=spark.read.option(\"multiline\",\"true\").json(\"C:\\\\Users\\\\DELL\\\\Downloads\\\\zipcodes.json\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9f35749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------+----------------------+-----+\n",
      "|_corrupt_record                                                                          |email                 |name |\n",
      "+-----------------------------------------------------------------------------------------+----------------------+-----+\n",
      "|{                                                                                        |NULL                  |NULL |\n",
      "|   \"ID\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\" ],                                              |NULL                  |NULL |\n",
      "|   \"Name\":[\"Rick\",\"Dan\",\"Michelle\",\"Ryan\",\"Gary\",\"Nina\",\"Simon\",\"Guru\" ],                |NULL                  |NULL |\n",
      "|   \"Salary\":[\"623.3\",\"515.2\",\"611\",\"729\",\"843.25\",\"578\",\"632.8\",\"722.5\" ],               |NULL                  |NULL |\n",
      "|   \"StartDate\":[ \"1/1/2012\",\"9/23/2013\",\"11/15/2014\",\"5/11/2014\",\"3/27/2015\",\"5/21/2013\",|NULL                  |NULL |\n",
      "|      \"7/30/2013\",\"6/17/2014\"],                                                          |NULL                  |NULL |\n",
      "|   \"Dept\":[ \"IT\",\"Operations\",\"IT\",\"HR\",\"Finance\",\"IT\",\"Operations\",\"Finance\"]           |NULL                  |NULL |\n",
      "|}                                                                                        |NULL                  |NULL |\n",
      "|{\"employees\":[                                                                           |NULL                  |NULL |\n",
      "|NULL                                                                                     |shyamjaiswal@gmail.com|Shyam|\n",
      "|NULL                                                                                     |bob32@gmail.com       |Bob  |\n",
      "|NULL                                                                                     |jai87@gmail.com       |Jai  |\n",
      "|]}                                                                                       |NULL                  |NULL |\n",
      "+-----------------------------------------------------------------------------------------+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Reading multiple files at a time\n",
    "\n",
    "df=spark.read.json([\"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Data Engineering Python\\\\DEPython\\\\jsondata.json\",\"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Data Engineering Python\\\\DEPython\\\\data1.json\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "710fe9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: integer (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| NULL|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| NULL|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| NULL|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| NULL|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| NULL|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        false|           2126|               4053| 122396986|         NULL|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| NULL|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        false|           2126|               4053| 122396986|         NULL|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| NULL|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| NULL|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84| NULL|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14374|              25446| 471000465|         NULL|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| NULL|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        false|           3922|               7443| 133112149|         NULL|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| NULL|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| NULL|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        false|           1207|               2190|  36395913|         NULL|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| NULL|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| NULL|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| NULL|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| NULL|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| NULL|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        false|           4046|               7845| 172127599|         NULL|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| NULL|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        false|            610|               1209|  18525517|         NULL|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| NULL|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        false|            842|               1666|  28876493|         NULL|\n",
      "|       76512|  27203|   STANDARD|           ASHEBORO|   NC|       PRIMARY|35.71| -79.81| NULL|-0.79| 0.58|         NA|     US|        Asheboro, NC|   NA-US-NC-ASHEBORO|        false|           8355|              15228| 215474318|         NULL|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) Reading files with a user-specified custom schema\n",
    "\n",
    "# Define custom schema\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, BooleanType\n",
    "schema = StructType([\n",
    "      StructField(\"RecordNumber\",IntegerType(),True),\n",
    "      StructField(\"Zipcode\",IntegerType(),True),\n",
    "      StructField(\"ZipCodeType\",StringType(),True),\n",
    "      StructField(\"City\",StringType(),True),\n",
    "      StructField(\"State\",StringType(),True),\n",
    "      StructField(\"LocationType\",StringType(),True),\n",
    "      StructField(\"Lat\",DoubleType(),True),\n",
    "      StructField(\"Long\",DoubleType(),True),\n",
    "      StructField(\"Xaxis\",IntegerType(),True),\n",
    "      StructField(\"Yaxis\",DoubleType(),True),\n",
    "      StructField(\"Zaxis\",DoubleType(),True),\n",
    "      StructField(\"WorldRegion\",StringType(),True),\n",
    "      StructField(\"Country\",StringType(),True),\n",
    "      StructField(\"LocationText\",StringType(),True),\n",
    "      StructField(\"Location\",StringType(),True),\n",
    "      StructField(\"Decommisioned\",BooleanType(),True),\n",
    "      StructField(\"TaxReturnsFiled\",StringType(),True),\n",
    "      StructField(\"EstimatedPopulation\",IntegerType(),True),\n",
    "      StructField(\"TotalWages\",IntegerType(),True),\n",
    "      StructField(\"Notes\",StringType(),True)\n",
    "  ])\n",
    "\n",
    "df_with_schema = spark.read.schema(schema) \\\n",
    "        .json(\"C:\\\\Users\\\\DELL\\\\Downloads\\\\zipcodes.json\")\n",
    "df_with_schema.printSchema()\n",
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a7555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59e92ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a   b\n",
       "0  11  14\n",
       "1  12  15\n",
       "2  13  16"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions in pandas DF\n",
    "# 1) transform()\n",
    "\n",
    "#each function takes a pandas Series, and the pandas API on Spark computes the functions in a distributed manner \n",
    "\n",
    "# should return same length as output\n",
    "import pandas as pd\n",
    "psdf = pd.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "print(psdf)\n",
    "\n",
    "# Adding 10 to each element\n",
    "\n",
    "def pandas_plus(count):\n",
    "     return count + 10  # should always return the same length as input.\n",
    "    \n",
    "psdf.transform(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d601e27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  5\n",
       "2  3  7"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) apply\n",
    "\n",
    "\n",
    "#each function takes a pandas Series, and the pandas API on Spark computes the functions in a distributed manner \n",
    "\n",
    "# It is not mandatory to return same length as output\n",
    "\n",
    "psdf = pd.DataFrame({'a': [1,2,3], 'b':[5,6,7]})\n",
    "def pandas_plus(x):\n",
    "    return x[x % 2 == 1]  # allows an arbitrary length\n",
    "\n",
    "psdf.apply(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9445d81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n",
      "       Fee  Discount\n",
      "0  20000.0      1000\n",
      "1  25000.0      2500\n",
      "2  30000.0      1500\n",
      "3  22000.0      1200\n",
      "4      NaN      3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\pandas\\utils.py:1016: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fee         45000.0\n",
      "Discount     3500.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "technologies = ({\n",
    "    'Fee' :[20000,25000,30000,22000,np.NaN],\n",
    "    'Discount':[1000,2500,1500,1200,3000]\n",
    "               })\n",
    "\n",
    "psdf = pd.DataFrame(technologies)\n",
    "print(type(psdf))\n",
    "print(psdf)\n",
    "\n",
    "\n",
    "def add(data):\n",
    "   return data[0]+data[1]\n",
    "  \n",
    "addDF = psdf.apply(add)\n",
    "print(addDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a2246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e73b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
