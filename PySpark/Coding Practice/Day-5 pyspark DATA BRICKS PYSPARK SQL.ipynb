{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e81450-2c5d-41dc-b149-c6cb14392dbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Bricks\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Bricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958a2486-2db2-4597-adb3-dc79e541e04f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkCreateTableExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfbff3e-320e-4d5a-9e8d-a7dc26e7f6e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# creating a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS db;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a6f9ec-91e8-428e-8fcb-863463304d7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# creating a table Employee2\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS Employee2(emp_id Int, emp_name String,Salary INT,age INT,city String)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef65fc1d-3587-4bbb-96e5-c1abbeb12f0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"INSERT INTO Employee2 VALUES (1,'Sona',80000,22,'Hyderabad')\")\n",
    "spark.sql(\"insert into employee2 values(2,'Sunny',70000,23,'Pune'),(3,'Mona',90000,19,'Hyderabad'),(7,'Chinnu',45000,25,'Mumbai'),(10,'Bannu',12,27,'Pune'),(6,'Raju',70000,45,'Mumbai'),(18,'Dhana',89999,34,'Kolkata')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5b7088-e133-42ca-a126-83892a232af1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     2|   Sunny| 70000| 23|     Pune|\n|     3|    Mona| 90000| 19|Hyderabad|\n|     7|  Chinnu| 45000| 25|   Mumbai|\n|    10|   Bannu|    12| 27|     Pune|\n|     6|    Raju| 70000| 45|   Mumbai|\n|    18|   Dhana| 89999| 34|  Kolkata|\n|     1|    Sona| 80000| 22|Hyderabad|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select * from employee2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0374f3-66ee-4240-837b-280e8e20d002",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n|emp_id|emp_name|\n+------+--------+\n|     2|   Sunny|\n|     3|    Mona|\n|     7|  Chinnu|\n|    10|   Bannu|\n|     6|    Raju|\n|    18|   Dhana|\n|     1|    Sona|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# select required columns\n",
    "spark.sql(\"select emp_id,emp_name from employee2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c778a8-e025-4980-8cf4-b3d378e8d82a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n|emp_id|emp_name|\n+------+--------+\n|     2|   Sunny|\n|    10|   Bannu|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# filter using where clause\n",
    "spark.sql(\" select emp_id,emp_name from employee2 where city='Pune' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0a19f3-aea2-4346-9ddf-baa07030d6de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+----+\n|emp_id|emp_name|Salary|age|city|\n+------+--------+------+---+----+\n|    10|   Bannu|    12| 27|Pune|\n+------+--------+------+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from employee2 where city='Pune' and emp_id=10 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e12e37-9c64-4767-8e11-ebeb9fee1dbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|     city|city_count|\n+---------+----------+\n|   Mumbai|         2|\n|  Kolkata|         1|\n|     Pune|         2|\n|Hyderabad|         2|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# group by\n",
    "spark.sql(\"select city,count(*) as city_count from employee2 group by city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415a1156-9a71-40c7-bb07-8f7870d7da8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|Salary_sum|     city|\n+----------+---------+\n|    115000|   Mumbai|\n|     89999|  Kolkata|\n|     70012|     Pune|\n|    170000|Hyderabad|\n+----------+---------+\n\n+----------+---------+\n|Salary_Min|     city|\n+----------+---------+\n|     45000|   Mumbai|\n|     89999|  Kolkata|\n|        12|     Pune|\n|     80000|Hyderabad|\n+----------+---------+\n\n+----------+---------+\n|Salary_Max|     city|\n+----------+---------+\n|     70000|   Mumbai|\n|     89999|  Kolkata|\n|     70000|     Pune|\n|     90000|Hyderabad|\n+----------+---------+\n\n+----------+---------+\n|Salary_Avg|     city|\n+----------+---------+\n|   57500.0|   Mumbai|\n|   89999.0|  Kolkata|\n|   35006.0|     Pune|\n|   85000.0|Hyderabad|\n+----------+---------+\n\n+-----------+---------+\n|Salary_mean|     city|\n+-----------+---------+\n|    57500.0|   Mumbai|\n|    89999.0|  Kolkata|\n|    35006.0|     Pune|\n|    85000.0|Hyderabad|\n+-----------+---------+\n\n+---------+\n|Count_emp|\n+---------+\n|        7|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# aggregations\n",
    "# 1) sum()\n",
    "spark.sql(\"select sum(salary) as Salary_sum,city from employee2 group by city\").show()\n",
    "\n",
    "# 2) min()\n",
    "spark.sql(\"select min(salary) as Salary_Min,city from employee2 group by city\").show()\n",
    "\n",
    "# 3) max()\n",
    "spark.sql(\"select max(salary) as Salary_Max,city from employee2 group by city\").show()\n",
    "\n",
    "# 4) avg()\n",
    "spark.sql(\"select avg(salary) as Salary_Avg,city from employee2 group by city\").show()\n",
    "\n",
    "# 5) mean()\n",
    "spark.sql(\"select mean(salary) as Salary_mean,city from employee2 group by city \").show()\n",
    "\n",
    "# 6) count()\n",
    "spark.sql(\"select count(*) as Count_emp from employee2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21fec726-e5ec-45de-a9d5-fa599f264caf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     1|    Sona| 80000| 22|Hyderabad|\n|     3|    Mona| 90000| 19|Hyderabad|\n|    18|   Dhana| 89999| 34|  Kolkata|\n|     6|    Raju| 70000| 45|   Mumbai|\n|     7|  Chinnu| 45000| 25|   Mumbai|\n|     2|   Sunny| 70000| 23|     Pune|\n|    10|   Bannu|    12| 27|     Pune|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# order by\n",
    "spark.sql(\"select * from employee2 order by city,emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59e6370-8160-4740-90cf-d369888b4e07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     2|   Sunny| 70000| 23|     Pune|\n|     1|    Sona| 80000| 22|Hyderabad|\n|     6|    Raju| 70000| 45|   Mumbai|\n|     3|    Mona| 90000| 19|Hyderabad|\n|    18|   Dhana| 89999| 34|  Kolkata|\n|     7|  Chinnu| 45000| 25|   Mumbai|\n|    10|   Bannu|    12| 27|     Pune|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# in descending order\n",
    "spark.sql(\"select * from employee2 order by emp_name desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8ed010-cc90-40bb-8396-36cf23f46ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# Joins\n",
    "# create another table department\n",
    "spark.sql(\"create table if not exists Department(dept_id int,emp_id int,dept_name string)\")\n",
    "\n",
    "# insert records into department table\n",
    "spark.sql(\"insert into Department values(100,1,'IT'),(102,6,'HR'),(103,13,'Manager'),(109,7,'Developer'),(110,10,'Tester')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cf279f-ea2b-40ab-b37f-756db228f6ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n|dept_id|emp_id|dept_name|\n+-------+------+---------+\n|    100|     1|       IT|\n|    102|     6|       HR|\n|    103|    13|  Manager|\n|    109|     7|Developer|\n|    110|    10|   Tester|\n+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# records of department table\n",
    "spark.sql(\"select * from department\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8159b944-6f24-47bf-b8ae-5f915db2ab42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+-------+------+---------+\n|emp_id|emp_name|Salary|age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+---+---------+-------+------+---------+\n|     2|   Sunny| 70000| 23|     Pune|   null|  null|     null|\n|     3|    Mona| 90000| 19|Hyderabad|   null|  null|     null|\n|     7|  Chinnu| 45000| 25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12| 27|     Pune|    110|    10|   Tester|\n|     6|    Raju| 70000| 45|   Mumbai|    102|     6|       HR|\n|    18|   Dhana| 89999| 34|  Kolkata|   null|  null|     null|\n|     1|    Sona| 80000| 22|Hyderabad|    100|     1|       IT|\n+------+--------+------+---+---------+-------+------+---------+\n\n+------+--------+------+---+---------+-------+------+---------+\n|emp_id|emp_name|Salary|age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+---+---------+-------+------+---------+\n|     2|   Sunny| 70000| 23|     Pune|   null|  null|     null|\n|     3|    Mona| 90000| 19|Hyderabad|   null|  null|     null|\n|     7|  Chinnu| 45000| 25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12| 27|     Pune|    110|    10|   Tester|\n|     6|    Raju| 70000| 45|   Mumbai|    102|     6|       HR|\n|    18|   Dhana| 89999| 34|  Kolkata|   null|  null|     null|\n|     1|    Sona| 80000| 22|Hyderabad|    100|     1|       IT|\n+------+--------+------+---+---------+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left join\n",
    "spark.sql(\"select * from employee2 left join department on employee2.emp_id=department.emp_id\").show()\n",
    "\n",
    "spark.sql(\"select * from employee2 left outer join department on employee2.emp_id=department.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2abf676a-918c-4730-819c-33b2c23944fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----+---------+-------+------+---------+\n|emp_id|emp_name|Salary| age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+----+---------+-------+------+---------+\n|     1|    Sona| 80000|  22|Hyderabad|    100|     1|       IT|\n|     6|    Raju| 70000|  45|   Mumbai|    102|     6|       HR|\n|  null|    null|  null|null|     null|    103|    13|  Manager|\n|     7|  Chinnu| 45000|  25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12|  27|     Pune|    110|    10|   Tester|\n+------+--------+------+----+---------+-------+------+---------+\n\n+------+--------+------+----+---------+-------+------+---------+\n|emp_id|emp_name|Salary| age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+----+---------+-------+------+---------+\n|     1|    Sona| 80000|  22|Hyderabad|    100|     1|       IT|\n|     6|    Raju| 70000|  45|   Mumbai|    102|     6|       HR|\n|  null|    null|  null|null|     null|    103|    13|  Manager|\n|     7|  Chinnu| 45000|  25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12|  27|     Pune|    110|    10|   Tester|\n+------+--------+------+----+---------+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Right join\n",
    "spark.sql(\"select * from employee2 right join department on employee2.emp_id=department.emp_id\").show()\n",
    "\n",
    "spark.sql(\"select * from employee2 right outer join  department on employee2.emp_id=department.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7cbac1-f545-4673-bf84-ed00eaec12e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+-------+------+---------+\n|emp_id|emp_name|Salary|age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+---+---------+-------+------+---------+\n|     7|  Chinnu| 45000| 25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12| 27|     Pune|    110|    10|   Tester|\n|     6|    Raju| 70000| 45|   Mumbai|    102|     6|       HR|\n|     1|    Sona| 80000| 22|Hyderabad|    100|     1|       IT|\n+------+--------+------+---+---------+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Inner join\n",
    "spark.sql(\"select * from employee2  join  department on employee2.emp_id=department.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07fc8dd-916f-4e96-9d02-e8afabc0ff8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----+---------+-------+------+---------+\n|emp_id|emp_name|Salary| age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+----+---------+-------+------+---------+\n|     1|    Sona| 80000|  22|Hyderabad|    100|     1|       IT|\n|     2|   Sunny| 70000|  23|     Pune|   null|  null|     null|\n|     3|    Mona| 90000|  19|Hyderabad|   null|  null|     null|\n|     6|    Raju| 70000|  45|   Mumbai|    102|     6|       HR|\n|     7|  Chinnu| 45000|  25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12|  27|     Pune|    110|    10|   Tester|\n|  null|    null|  null|null|     null|    103|    13|  Manager|\n|    18|   Dhana| 89999|  34|  Kolkata|   null|  null|     null|\n+------+--------+------+----+---------+-------+------+---------+\n\n+------+--------+------+----+---------+-------+------+---------+\n|emp_id|emp_name|Salary| age|     city|dept_id|emp_id|dept_name|\n+------+--------+------+----+---------+-------+------+---------+\n|     1|    Sona| 80000|  22|Hyderabad|    100|     1|       IT|\n|     2|   Sunny| 70000|  23|     Pune|   null|  null|     null|\n|     3|    Mona| 90000|  19|Hyderabad|   null|  null|     null|\n|     6|    Raju| 70000|  45|   Mumbai|    102|     6|       HR|\n|     7|  Chinnu| 45000|  25|   Mumbai|    109|     7|Developer|\n|    10|   Bannu|    12|  27|     Pune|    110|    10|   Tester|\n|  null|    null|  null|null|     null|    103|    13|  Manager|\n|    18|   Dhana| 89999|  34|  Kolkata|   null|  null|     null|\n+------+--------+------+----+---------+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Full outer join\n",
    "spark.sql(\"select * from employee2  full outer join  department on employee2.emp_id=department.emp_id\").show()\n",
    "\n",
    "spark.sql(\"select * from employee2  full join  department on employee2.emp_id=department.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de698b6-dfa7-4aaf-87e8-e6f33fbe6109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     7|  Chinnu| 45000| 25|   Mumbai|\n|    10|   Bannu|    12| 27|     Pune|\n|     6|    Raju| 70000| 45|   Mumbai|\n|     1|    Sona| 80000| 22|Hyderabad|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left semi join\n",
    "spark.sql(\"select * from employee2  left semi join  department on employee2.emp_id=department.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4926d593-b760-4ba3-ac79-e4e17eb14c88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     2|   Sunny| 70000| 23|     Pune|\n|     3|    Mona| 90000| 19|Hyderabad|\n|    18|   Dhana| 89999| 34|  Kolkata|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# left anti\n",
    "spark.sql(\"select * from employee2  left anti join  department on employee2.emp_id=department.emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2e356f-b214-40bd-bcb5-26038e8cf905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     2|   Sunny| 70000| 23|     Pune|\n|     3|    Mona| 90000| 19|Hyderabad|\n|     7|  Chinnu| 45000| 25|   Mumbai|\n|    10|   Bannu|    12| 27|     Pune|\n|     6|    Raju| 70000| 45|   Mumbai|\n|    18|   Dhana| 89999| 34|  Kolkata|\n|     1|    Sona| 80000| 22|Hyderabad|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Creating a temporary view\n",
    "\n",
    "# Creating Employee2  DataFrame containing the data of employees\n",
    "Employee2 = spark.sql(\"SELECT * FROM Employee2\")\n",
    "print(type(Employee2))\n",
    "\n",
    "# Create a temporary view from the DataFrame\n",
    "Employee2.createOrReplaceTempView(\"employee2_view\")\n",
    "\n",
    "# Example query using the temporary view\n",
    "spark.sql(\"SELECT * FROM employee2_view\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e8dbf7-4fa4-46a1-aa0b-20cb23477733",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n|emp_id|emp_name|\n+------+--------+\n|     2|   Sunny|\n|     3|    Mona|\n|     7|  Chinnu|\n|    10|   Bannu|\n|     6|    Raju|\n|    18|   Dhana|\n|     1|    Sona|\n+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# selecting required columns from temporary view\n",
    "spark.sql(\"select emp_id,emp_name from employee2_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746ade91-6c77-4187-bd7e-30566ad7feed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+\n|emp_name|Salary|age|\n+--------+------+---+\n|    Mona| 90000| 19|\n+--------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# applying condition on temporary view that is created\n",
    "\n",
    "spark.sql(\" select emp_name,Salary,age from employee2_view where emp_id=3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007f1faf-1286-4ac1-b13f-19fd628882e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+---------+\n|emp_id|emp_name|Salary|age|     city|\n+------+--------+------+---+---------+\n|     3|    Mona| 90000| 19|Hyderabad|\n|     1|    Sona| 80000| 22|Hyderabad|\n+------+--------+------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from employee2_view where emp_name like '%on%' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de49a605-5279-4a01-b1d0-48b245a9e9d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+\n|emp_id|emp_name| rn|\n+------+--------+---+\n|     1|    Sona|  1|\n|     3|    Mona|  2|\n|    18|   Dhana|  1|\n|     7|  Chinnu|  1|\n|     6|    Raju|  2|\n|    10|   Bannu|  1|\n|     2|   Sunny|  2|\n+------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Window functions\n",
    "# i) ROW_NUMBER()\n",
    "# with partition\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_id, emp_name,\n",
    "           ROW_NUMBER() OVER(PARTITION BY city ORDER BY salary) AS rn \n",
    "    FROM employee2_view\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdc266c-4150-49d6-917a-06dce253415a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+\n|emp_id|emp_name| rn|\n+------+--------+---+\n|    10|   Bannu|  1|\n|     7|  Chinnu|  2|\n|     2|   Sunny|  3|\n|     6|    Raju|  4|\n|     1|    Sona|  5|\n|    18|   Dhana|  6|\n|     3|    Mona|  7|\n+------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# without partition\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_id, emp_name,\n",
    "           ROW_NUMBER() OVER(ORDER BY salary) AS rn \n",
    "    FROM employee2_view\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b606f1a-f009-44bc-9503-1b74eba39178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n|emp_id|emp_name|rank|\n+------+--------+----+\n|     1|    Sona|   1|\n|     3|    Mona|   2|\n|    18|   Dhana|   1|\n|     7|  Chinnu|   1|\n|     6|    Raju|   2|\n|    10|   Bannu|   1|\n|     2|   Sunny|   2|\n+------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ii) RANK()\n",
    "# with partition\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_id, emp_name,\n",
    "           RANK() OVER(PARTITION BY city ORDER BY salary) AS rank \n",
    "    FROM employee2_view\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0578d7f-6bb0-4ecc-bc31-5f13f80585c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n|emp_id|emp_name|rank|\n+------+--------+----+\n|    10|   Bannu|   1|\n|     7|  Chinnu|   2|\n|     2|   Sunny|   3|\n|     6|    Raju|   3|\n|     1|    Sona|   5|\n|    18|   Dhana|   6|\n|     3|    Mona|   7|\n+------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# without partition\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_id, emp_name,\n",
    "           RANK() OVER( ORDER BY salary) AS rank \n",
    "    FROM employee2_view\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2fd0cf-25ae-4837-bebe-3080b6f08ac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n|emp_id|emp_name|rank|\n+------+--------+----+\n|     1|    Sona|   1|\n|     3|    Mona|   2|\n|    18|   Dhana|   1|\n|     7|  Chinnu|   1|\n|     6|    Raju|   2|\n|    10|   Bannu|   1|\n|     2|   Sunny|   2|\n+------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# iii) DENSE_RANK()\n",
    "# with partition\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_id, emp_name,\n",
    "           DENSE_RANK() OVER(PARTITION BY city ORDER BY salary) AS rank \n",
    "    FROM employee2_view\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb1e0fa-9665-481f-9489-f1afc382f330",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n|emp_id|emp_name|rank|\n+------+--------+----+\n|    10|   Bannu|   1|\n|     7|  Chinnu|   2|\n|     2|   Sunny|   3|\n|     6|    Raju|   3|\n|     1|    Sona|   4|\n|    18|   Dhana|   5|\n|     3|    Mona|   6|\n+------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# iii) DENSE_RANK()\n",
    "# without partition\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_id, emp_name,\n",
    "           DENSE_RANK() OVER(ORDER BY salary) AS rank \n",
    "    FROM employee2_view\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47049a08-269d-40c5-854d-ba72a5d93849",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n|emp_id|emp_name|rank|\n+------+--------+----+\n|     1|    Sona|   1|\n|     3|    Mona|   2|\n|    18|   Dhana|   1|\n|     7|  Chinnu|   1|\n|     6|    Raju|   2|\n|    10|   Bannu|   1|\n|     2|   Sunny|   2|\n+------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# iv) NTILE(N)\n",
    "# with partition\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_id, emp_name,\n",
    "           ntile(2) OVER(PARTITION BY city ORDER BY salary) AS rank \n",
    "    FROM employee2_view\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9995d286-98fb-4f63-bb9f-74846dbe16f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n|emp_id|emp_name|rank|\n+------+--------+----+\n|    10|   Bannu|   1|\n|     7|  Chinnu|   1|\n|     2|   Sunny|   1|\n|     6|    Raju|   1|\n|     1|    Sona|   2|\n|    18|   Dhana|   2|\n|     3|    Mona|   2|\n+------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# iv) NTILE(N)\n",
    "# without partition\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT emp_id, emp_name,\n",
    "           ntile(2) OVER( ORDER BY salary) AS rank \n",
    "    FROM employee2_view\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d92aae-0685-464e-a90c-8eb8bfe84f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+----------+----------+----------+\n|emp_id|emp_name|max_salary|min_salary|avg_salary|sum_salary|\n+------+--------+----------+----------+----------+----------+\n|     3|    Mona|     90000|     80000|   85000.0|    170000|\n|     1|    Sona|     90000|     80000|   85000.0|    170000|\n|    18|   Dhana|     89999|     89999|   89999.0|     89999|\n|     7|  Chinnu|     70000|     45000|   57500.0|    115000|\n|     6|    Raju|     70000|     45000|   57500.0|    115000|\n|     2|   Sunny|     70000|        12|   35006.0|     70012|\n|    10|   Bannu|     70000|        12|   35006.0|     70012|\n+------+--------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregate functions in Window Functions\n",
    "spark.sql(\"\"\" SELECT emp_id,emp_name,\n",
    " MAX(salary) over(partition by city) as max_salary,\n",
    " MIN(salary) over(partition by city) as min_salary,\n",
    " AVG(salary) over(partition by city) as avg_salary,\n",
    " SUM(salary) over(PARTITION BY city) as sum_salary\n",
    " FROM employee2_view\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day-5 pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
